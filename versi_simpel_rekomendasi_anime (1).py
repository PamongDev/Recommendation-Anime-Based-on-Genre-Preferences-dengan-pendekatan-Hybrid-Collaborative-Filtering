# -*- coding: utf-8 -*-
"""Versi Simpel Rekomendasi Anime.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MtftyHYi92zaev-5phKK_lEwrNGfn673
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Library"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
import kagglehub
import os
import shutil
from sklearn.cluster import KMeans
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow.keras.models import load_model, Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dot
from tensorflow.keras.optimizers import Adam

"""# Data Understanding

Melakukan download data dari `Kaggle` dan memindahkan dataset menuju `/content`
"""

# Download latest version
path = kagglehub.dataset_download("dbdmobile/myanimelist-dataset")

print("Original Path to dataset files:", path)

# Get the destination directory
destination_dir = "/content"

# Iterate through the files and directories within the downloaded dataset
for item in os.listdir(path):
    source_item = os.path.join(path, item)
    destination_item = os.path.join(destination_dir, item)

    if os.path.isdir(source_item):
        # For directories, use shutil.copytree to preserve directory structure
        shutil.copytree(source_item, destination_item, dirs_exist_ok=True)
    else:
        # For files, use shutil.copy2 to preserve metadata
        shutil.copy2(source_item, destination_item)

print(f"Files moved to: {destination_dir}")

data_path = os.listdir("/content")[1:-2]
data_path

"""## Information Data

Melihat Informasi setiap data

### Data Anime
"""

anime_df = pd.read_csv('anime-dataset-2023.csv')
anime_df.info()

anime_filter_df = pd.read_csv('anime-filtered.csv')
anime_filter_df.info()

"""### Data User Rating"""

user_rating_df = pd.read_csv('user-filtered.csv')
user_rating_df.info(show_counts=True)

user_score_df = pd.read_csv( 'users-score-2023.csv')
user_score_df.info(show_counts=True)

"""### Data User Details"""

user_details_df = pd.read_csv('users-details-2023.csv')
user_details_df.info(show_counts=True)

"""### Data Final Anime dataset"""

final_anime_df = pd.read_csv('final_animedataset.csv')
final_anime_df.info(show_counts=True)

"""### Selected Data
Berdasarkan data yang ditampilkan, tentu untuk melakukan rekomendasi tidak perlu menggunakan semua variabel yang ada pada setiap data. Saya memfokuskan variabel data berupa `user_id`, `anime_id`, `rating`, `Name` atau `anime_title`, serta `genre`.

Oleh karena itu, penulis hanya menggunakan data yang tentunya nanti akan dipilah fitur sesuai dengan variabel yang akan digunakan seperti berikut ini:
- `anime_df`
- `user_rating_df`

## Number of data for each variable
"""

print(f'Jumlah data anime = {len(anime_df)}')
print(f'Jumlah data user rating = {len(user_rating_df.user_id.unique())}')
print(f'Jumlah data anime yang dirating user = {len(user_rating_df.anime_id.unique())}')

"""## EDA TOP ANIME and TOP USER

Melihat Top 20 User yang melakukan rating pada Anime_id
"""

top_user_rating = user_rating_df.groupby('user_id')['rating'].count().dropna().sort_values(ascending=False)[:20]
top_user_rating.head(20)

"""Melihat Top 20 anime_id yang di rating oleh user_id"""

top_anime_rating = user_rating_df.groupby('anime_id')['rating'].count().dropna().sort_values(ascending=False)[:20]
top_anime_rating.head(20)

"""Menghubungkan antara top user rating dengan data user rating"""

top_relation_anime_user = user_rating_df.join(top_user_rating, rsuffix='_r', how='inner', on='user_id')
top_relation_anime_user.info()

"""menghubungkan antara top anime rating dengan top user_id sebelumnya"""

top_relation_anime_user = top_relation_anime_user.join(top_anime_rating, rsuffix='_r', how='inner', on='anime_id')
top_relation_anime_user.info()

"""Representasi TOP ANIME dan TOP USER dalam matrix. Dalam hal ini, kita dapat melihat bahwa dari top 20 anime, tidak semua top user melakukan rating. Hanya sekitar `8 top user` dari 20 top user yang melakukan rating dari top 20 anime."""

pd.set_option('display.max_columns', None)
pd.crosstab(top_relation_anime_user.user_id, top_relation_anime_user.anime_id, top_relation_anime_user.rating, aggfunc=np.sum)

"""# Data Preprocessing

## Check Noise Data

### Check Duplicate Data
Melakukan pengeccekan Duplikat Setiap Data
"""

anime_df.duplicated().sum()

user_rating_df.duplicated().sum()

"""### Solve Duplicated Data
Mengatasi data duplikat data duplikat
"""

user_rating_df.drop_duplicates(inplace=True)
user_rating_df.duplicated().sum()

"""### Check Missing Value"""

anime_df.isnull().sum().sum()

user_rating_df.isnull().sum().sum()

"""## Selected Feature on Anime Data
Memilih fitur atau variabel pada Anime data, variabel yang digunakan yaitu :
- `anime_id`
- `Name`
- `Genres`
"""

selected_feature = ['anime_id', 'Name', 'Genres']
anime_df = anime_df[selected_feature]

"""## one hot encoding Genres on Anime Data
Hal ini dapat membantu memecahkan data genre menjadi sebuah kolom yang bernilai boolean `(0 or 1)` sehingga data genre dapat diproses dalam sistem
"""

anime_df['Genres'] = (
    anime_df['Genres']
    .str.lower()                   # Ubah ke huruf kecil
    .str.replace(' ', '', regex=False)  # Hapus spasi
    .str.split(',')                # Split by ','
)

# One-hot encoding
mlb = MultiLabelBinarizer()
genres_encoded = pd.DataFrame(
    mlb.fit_transform(anime_df['Genres']),
    columns=mlb.classes_,
    index=anime_df.index
)

# Gabungkan dengan tabel asli
anime_df = pd.concat([anime_df, genres_encoded], axis=1)
anime_df.info()

"""## Create data Limit on User Rating
Saya membuat batasan data pada user rating. User yang dipilih adalah user yang minimal melakukan `500` rating data Anime.
"""

n_ratings = user_rating_df.user_id.value_counts()
get_index =  n_ratings[n_ratings >= 500].index
user_rating_df = user_rating_df[user_rating_df['user_id'].isin(get_index)].copy()
user_rating_df.info(show_counts=True)

print("Jumlah user yang rating saat ini", len(get_index))
print("Jumlah Anime yang rating saat ini", len(user_rating_df.anime_id.unique()))

"""## Build Encode Decode anime_id and user_id

Hal ini membuat user_id dan anime_id kedalam numerik dengan rentang `0 - jumlah user_id` dan `0 - anime_id`, yang membantu mengoptimalkan proses komputasi dan mempermudah pengolahan data
"""

# encode-decode data anime
anime_index = user_rating_df.anime_id.unique().tolist()
anime_encode = {x: i for i, x in enumerate(anime_index)}
anime_decode = {i: x for i, x in enumerate(anime_index)}

# encode-decode data user
user_index = user_rating_df.user_id.unique().tolist()
user_encode = {x: i for i, x in enumerate(user_index)}
user_decode = {i: x for i, x in enumerate(user_index)}

# mapping encode data anime dan data user
user_rating_df['anime_id'] = user_rating_df['anime_id'].map(anime_encode)
user_rating_df['user_id'] = user_rating_df['user_id'].map(user_encode)

print(f"Jumlah user = {len(user_index)}")
print(f"Jumlah anime = {len(anime_index)}")

"""## Normalisasi Rating data

mengubah rentang data menjadi `0-1`
"""

user_rating_df.rating.unique()

min_rating = min(user_rating_df['rating'])
max_rating = max(user_rating_df['rating'])
user_rating_df['rating'] = user_rating_df["rating"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values.astype(np.float64)

user_rating_df.describe()

"""## Training Test set split

Membagi data menjadi data training dan data testing
"""

# Pembagian data
train, test = train_test_split(user_rating_df, test_size=0.2, random_state=42)

# Training set
train_users = train["user_id"].values
train_animes = train["anime_id"].values
train_ratings = train["rating"].values

# Testing set
test_users = test["user_id"].values
test_animes = test["anime_id"].values
test_ratings = test["rating"].values

"""# Modelling
Membangun model embedding
"""

def EmbeddingModel(n_users, n_animes, embedding_dim=128):
    # Input untuk user
    user_input = Input(shape=(1,), name="user_input")
    user_embedding = Embedding(input_dim=n_users, output_dim=embedding_dim, name="user_embedding")(user_input)
    user_vec = Flatten()(user_embedding)

    # Input untuk anime
    anime_input = Input(shape=(1,), name="anime_input")
    anime_embedding = Embedding(input_dim=n_animes, output_dim=embedding_dim, name="anime_embedding")(anime_input)
    anime_vec = Flatten()(anime_embedding)

    # Dot product antara user dan anime
    dot_product = Dot(axes=1)([user_vec, anime_vec])

    # Model
    model = Model(inputs=[user_input, anime_input], outputs=dot_product)
    model.compile(optimizer=Adam(learning_rate=0.001), loss="mean_squared_error")
    return model

# Inisialisasi model
embedding_model = EmbeddingModel(len(user_encode), len(anime_encode))

"""## Setup TPU

untuk melakukan training embedding, perlu mengatur TPU agar proses lebih cepat dibandingkan dengan CPU
"""

# prompt: tensorflow connect TPU
try:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
  # print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])
except ValueError:
  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.TPUStrategy(tpu)

"""## Training Embedding Model

Melakukan training dengan TPU strategy, karena menggunakan TPU penulis memperbesarkan `batch_size = 3096` dan TPU yang saya gunakan terbatas, saya menggunakan `epoch = 10`. Hal ini dilakukan karena data begitu besar dengan device terbatas, sehingga saya mengakali nya demikian.
"""

with tpu_strategy.scope():
    # Your existing model definition here
    embedding_model = EmbeddingModel(len(user_encode), len(anime_encode))
    embedding_model.summary()

    # ... rest of your training code ...
    history = embedding_model.fit(
    [train_users, train_animes],
    train_ratings,
    batch_size=3096,
    epochs=10,
    validation_data = ([test_users, test_animes], test_ratings),
    verbose=1)

"""menampilkan grafik training loss dan testing loss saat proses Training. MSE yang didapat adalah `0.0751`. Nilai loss seperti ini sudah layak untuk digunakan untuk Sistem Rekomendasi."""

# Plot training & validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Testing Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss During Training')
plt.legend()
plt.show()

"""## Save model

melakukan save model agar menghemat waktu untuk evaluasi
"""

embedding_model.save('embedding_model.h5')

"""## Load Model"""

embedding_path = '/content/drive/MyDrive/Colab Notebooks/embedding_model.h5'

# Load the saved model
embedding_model = load_model(embedding_path)
embedding_model.summary()

"""## Extract Weights Model Embedding

hasil ekstrak bobot ini akan digunakan untuk memprediksi rating daalam Sistem Rekomendasi
"""

def extract_weights(name, model):
    weight_layer = model.get_layer(name)
    weights = weight_layer.get_weights()[0]
    weights = weights / np.linalg.norm(weights, axis = 1).reshape((-1, 1))
    return weights

anime_weights = extract_weights('anime_embedding', embedding_model)
user_weights = extract_weights('user_embedding', embedding_model)

"""# Prediction Evaluation

melihat evaluasi prediksi rating pada data testing menggunakan MSE
"""

predicted_ratings = embedding_model.predict([test_users, test_animes])  # Rating prediksi

mse = mean_squared_error(test_ratings, predicted_ratings)
print(f"Mean Squared Error: {mse}")

"""# System Recommendation

## Function Find Similar User

mencari kemiripan dari setiap user
"""

def find_similar_users(user_id, n=10, return_dist=False, neg=False):
  try:
        # Mendapatkan user_id encode
        encoded_user = user_encode.get(user_id)

        # Mengambil bobot embedding user
        weights = user_weights

        # Menghitung jarak antara user menggunakan dot product
        dists = np.dot(weights, weights[encoded_user])

        # Mengurutkan berdasarkan jarak (dari yang terjauh ke yang terdekat)
        sorted_dists = np.argsort(dists)

        # Menentukan jumlah user id yang ingin dicari
        n = n + 1

        if neg:
            closest = sorted_dists[:n]
        else:
            closest = sorted_dists[-n:]

        if return_dist:
            return dists, closest

        # Mengambil data anime dan memasukkan informasi ke dalam array
        SimilarityArr = []

        for close in closest:
            decoded_id = user_decode.get(close)

            similarity = dists[close]

            SimilarityArr.append({
                "simillar_usr": decoded_id,
                "similarity": similarity,
            })

        # Mengubah hasil ke dalam DataFrame dan mengurutkannya berdasarkan similarity
        Frame = pd.DataFrame(SimilarityArr).sort_values(by="similarity", ascending=False)

        # Mengembalikan hasil tanpa anime yang dicari
        return Frame[1:]

  except Exception as e:
        print(f"Error: {str(e)}")

similar_users = find_similar_users(11165, n=10)
similar_users

"""## Function Find Favorite Genres User
mendapatkan top 10 genre favorite user
"""

def get_favorite_genres_user(user_id, rating_data, anime_data):

    # choice anime id by user id
    select_anime = rating_data[rating_data['user_id'] == user_id].anime_id.unique()
    anime_favorite = anime_data[anime_data['anime_id'].isin(select_anime)]

    genre_col = anime_favorite.columns[3:]
    anime_favorite[genre_col].sum().sort_values(ascending=False).head(10).index

    return anime_favorite[genre_col].sum().sort_values(ascending=False).head(10)

get_favorite_genres_user(11165, user_rating_df, anime_df)

"""## Function Another favorite genre prediction

Memprediksi favorit genre lainnya dari kemiripan user
"""

def get_favorite_genres(similar_users, rating_data, anime_data):
    # Filter ratings by similar users
    user_ratings = rating_data[rating_data['user_id'].isin(similar_users['simillar_usr'].values)]

    # Merge with anime data to get genre information (one-hot encoded genres)
    merged_data = user_ratings.merge(anime_data, on='anime_id')

    # Calculate average rating per genre (considering one-hot encoding)
    genre_columns = [col for col in anime_data.columns if col not in ['anime_id', 'Name', 'Genres']]

    #convert to float
    merged_data[genre_columns] = merged_data[genre_columns].astype(float)

    # Multiply rating by one-hot encoded genre values, sum and average per genre
    genre_ratings = merged_data[genre_columns].multiply(merged_data['rating'], axis=0)

    # Calculate average rating per genre
    genre_preference = genre_ratings.sum(axis=0) / merged_data[genre_columns].sum(axis=0)

    # Sort genres by rating preference (highest to lowest)
    genre_preference = genre_preference.sort_values(ascending=False)

    return genre_preference

"""contoh genre favorite lainnya pada user_id = 11165"""

# Example usage:
similar_users = find_similar_users(11165, n=10)
genre_preference = get_favorite_genres(similar_users, user_rating_df, anime_df)
genre_preference

"""## Recommendation Anime Based on Genre preferences User Similar(Hybrid Collaborative Filtering)"""

def recommend_anime_for_user(user_id, rating_data, anime_data, top_n=20, sort_by='average_rating'):

    # get favorite genre by user_id
    favorite_genres_user = get_favorite_genres_user(user_id, rating_data, anime_data)

    # get simillar user and find other favorite genres
    similar_users = find_similar_users(user_id, n=top_n)
    favorite_genres = get_favorite_genres(similar_users, rating_data, anime_data)

    # Avoid anime favorite genres
    favorite_genres = favorite_genres[~favorite_genres.index.isin(favorite_genres_user.index)]

    # Filter anime data by favorite genres (one-hot encoded)
    favorite_genre_columns = favorite_genres.index[:3]  # Select top 3 favorite genres
    genre_filter = anime_data[favorite_genre_columns].sum(axis=1) > 0 # At least one favorite genre
    filtered_anime = anime_data[genre_filter]

    # Get anime already rated by the user
    rated_data = rating_data[rating_data['user_id'] == user_id]
    rated_anime = rated_data.anime_id.unique()

    unrated_anime = rating_data[~rating_data['anime_id'].isin(rated_anime)]

    # Exclude already rated anime
    recommendations = filtered_anime[~filtered_anime['anime_id'].isin(rated_anime)]

     # calce average rating
    # Hitung rata-rata rating untuk setiap anime
    anime_avg_rating = unrated_anime.groupby('anime_id')['rating'].mean().reset_index()
    anime_avg_rating.rename(columns={'rating': 'average_rating'}, inplace=True)

    # Gabungkan dengan dataset anime
    recommendations = recommendations.merge(anime_avg_rating, on='anime_id', how='left')

    # Hitung jumlah pengguna yang memberikan rating untuk setiap anime
    anime_popularity = unrated_anime.groupby('anime_id')['user_id'].count().reset_index()
    anime_popularity.rename(columns={'user_id': 'popularity'}, inplace=True)

    # Gabungkan dengan dataset anime
    recommendations = recommendations.merge(anime_popularity, on='anime_id', how='left')

    # Sort by a relevant metric (e.g., average rating or popularity)
    if sort_by == 'average_rating':
        recommendations = recommendations.sort_values(by='average_rating', ascending=False)
    elif sort_by == 'popularity':
        recommendations = recommendations.sort_values(by='popularity', ascending=False)

    set_columns = ['anime_id', 'Name', 'Genres', 'average_rating', 'popularity']
    # Return the top N recommendations
    return recommendations[set_columns].head(top_n).reset_index(drop=True)

# Example usage:
user_id = 11165  # Replace with actual user_id

recommended_anime = recommend_anime_for_user(
    user_id=user_id,
    rating_data=user_rating_df,
    anime_data=anime_df,
    top_n=20,
    sort_by='average_rating'
)
recommended_anime

"""# Evaluasi

## Evaluasi Rekomendasi Hybrid

membuat evaluasi dengan `precision`, `diversity`, dan `NDCG`
"""

def precision_at_k(recommended_items, relevant_items, k=10):
    recommended_items = recommended_items[:k]  # Ambil top k rekomendasi
    relevant_items = set(relevant_items)  # Set relevansi untuk mempermudah pencocokan
    relevant_count = sum(1 for item in recommended_items if item in relevant_items)
    return relevant_count / k

def calculate_diversity(recommended_items, cosine_sim=cosine_similarity(mlb.fit_transform(anime_df['Genres']))):
    # Mengambil indeks dari anime yang direkomendasikan
    recommended_indices = [anime_df[anime_df['anime_id'] == item].index[0] for item in recommended_items]

    # Mengambil cosine similarity antar anime yang direkomendasikan
    sim_matrix = cosine_sim[recommended_indices, :][:, recommended_indices]

    # Menghitung rata-rata similarity, kemudian dikurangkan dari 1 untuk mendapatkan diversity
    mean_similarity = np.mean(sim_matrix)
    diversity = 1 - mean_similarity
    return diversity

def ndcg_at_k(recommended_items, relevant_items, k=10):
    recommended_items = recommended_items[:k]
    dcg = 0
    idcg = 0

    # Hitung DCG (Discounted Cumulative Gain)
    for i, item in enumerate(recommended_items):
        if item in relevant_items:
            dcg += 1 / np.log2(i + 2)  # Menambahkan relevansi terdiskon

    # Pastikan hanya item yang ada di recommended_items yang dipertimbangkan untuk IDCG
    relevant_in_recommended = [item for item in relevant_items if item in recommended_items]

    # Hitung IDCG (Ideal DCG) yaitu DCG pada ranking yang ideal
    for i, item in enumerate(relevant_in_recommended[:k]):
        idcg += 1 / np.log2(i + 2)

    # Menghindari pembagian dengan nol
    if idcg == 0:
        return 0
    return dcg / idcg

"""Melakukan inisialisasi dengan memberikan `top k = 20` serta list untuk menampung evaluasi sistem rekomendasi, serta menampung user unique dari data `test_user`. Sampel yang akan di evaluasi sebanyak `100` sample"""

k = 20

all_precision = []
all_diversity = []
all_ndcg = []
user_unique = test['user_id'].unique()[:300]

"""Melakukan evaluasi sistem rekomendasi setiap user dengan metrik yang telah ditentukan"""

for i,user_index in enumerate(user_unique):
    relevant_items = test[test['user_id'] == user_index]['anime_id'].tolist()
    recommended_items = recommend_anime_for_user(user_index, train, anime_df).head(k)['anime_id'].tolist()

    precision = precision_at_k(recommended_items, relevant_items, k)
    diversity = calculate_diversity(recommended_items)
    ndcg = ndcg_at_k(recommended_items, relevant_items, k)

    all_precision.append(precision)
    all_diversity.append(diversity)
    all_ndcg.append(ndcg)

    print(f"proses user ke -{i+1} user = {user_index} precision = {precision} diversity = {diversity} ndcg = {ndcg}")

print(f"Precision{k}: {np.mean(all_precision)}")
print(f"Diversity@{k}: {np.mean(all_diversity)}")
print(f"NDCG@{k}: {np.mean(all_ndcg)}")

"""## Visualisasi Evaluasi setiap metrik"""

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.histplot(all_ndcg, kde=True, bins=10, color='blue')
plt.title('Distribusi NDCG')
plt.xlabel('NDCG')
plt.ylabel('Frekuensi')

plt.subplot(1, 3, 2)
sns.histplot(all_precision, kde=True, bins=10, color='green')
plt.title('Distribusi Precision')
plt.xlabel('Precision')
plt.ylabel('Frekuensi')

plt.subplot(1, 3, 3)
sns.histplot(all_diversity, kde=True, bins=10, color='purple')
plt.title('Distribusi Diversity')
plt.xlabel('Diversity')
plt.ylabel('Frekuensi')

plt.tight_layout()
plt.show()